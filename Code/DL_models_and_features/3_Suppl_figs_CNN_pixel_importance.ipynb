{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004a047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "number_raster_layers = 9\n",
    "number_pixels_layer = 19\n",
    "\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(number_raster_layers, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu1 = nn.SELU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu2 = nn.SELU()\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu3 = nn.SELU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu4 = nn.SELU()\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu5 = nn.SELU()\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu6 = nn.SELU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(512 * number_pixels_layer * number_pixels_layer, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.selu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.selu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.selu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.selu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.selu5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.selu6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = CNNRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "path_model = '../../Data/Calibrated_models/global_regressor_V0.pth'\n",
    "model.load_state_dict(torch.load(path_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cfeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambia el modelo al modo de evaluaci√≥n (si es necesario)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12456b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensors\n",
    "path_tensor_train = '../../Data/Calibrated_models/global_regressor_V0_tensor_y_train.pth'\n",
    "y_train = torch.load(path_tensor_train)\n",
    "\n",
    "path_tensor_test = '../../Data/Calibrated_models/global_regressor_V0_tensor_y_test.pth'\n",
    "y_test = torch.load(path_tensor_test)\n",
    "\n",
    "path_tensor_test = '../../Data/Calibrated_models/global_regressor_V0_test_tensor.pth'\n",
    "test_tensor = torch.load(path_tensor_test)\n",
    "\n",
    "path_tensor_training = '../../Data/Calibrated_models/global_regressor_V0_training_tensor.pth'\n",
    "training_tensor = torch.load(path_tensor_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f80e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of test sites\n",
    "total_number_test_sites = test_tensor.shape[0]\n",
    "print(total_number_test_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some sites to estimate the CNN importance for their pixels: 13, 81, 90, 132."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075207c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets show the results for site \"90\"\n",
    "input_tensor_number = 90 # 13, 81, 90, 132\n",
    "input_tensor_raw = test_tensor[input_tensor_number].unsqueeze(0) #9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input tensor to float to manipulate it with the CNN\n",
    "input_tensor = input_tensor_raw.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a6f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass to obtain the CNN activations\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224c06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0900e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Land cover names\n",
    "layer_names = [\n",
    "    \"a) Bare (%)\", \"b) Built-Up (%)\", \"c) Crops (%)\", \"d) Grass (%)\", \"e) Moss/Lichen (%)\",\n",
    "    \"f) Permanent water (%)\", \"g) Seasonal water (%)\", \"h) Shrub (%)\", \"i) Tree (%)\"\n",
    "]\n",
    "\n",
    "# Create a plot to show each land cover layer\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "\n",
    "# Iterate over each layer\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # show land cover image\n",
    "    im = ax.imshow(input_tensor[0, i, :, :], cmap='viridis')\n",
    "    ax.set_title(layer_names[i], fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create a new color bar.\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "    # Add color bar.\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save image\n",
    "fig.savefig('../../Figures/figA2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb \n",
    "\n",
    "input_tensor_squeeze = input_tensor.squeeze(0)\n",
    "\n",
    "# Define land cover color palette\n",
    "color_map = {\n",
    "    0: 'saddlebrown',   # Bare\n",
    "    1: 'red',           # BuiltUp\n",
    "    2: 'yellow',        # Crops\n",
    "    3: 'lightgreen',    # Grass\n",
    "    4: 'limegreen',     # MossLichen\n",
    "    5: 'blue',          # PermanentWater\n",
    "    6: 'cyan',          # SeasonalWater\n",
    "    7: 'olive',         # Shrub\n",
    "    8: 'darkgreen'      # Tree\n",
    "}\n",
    "\n",
    "# Estimate the land cover dominant layer\n",
    "dominant_layer_indices = np.argmax(input_tensor_squeeze, axis=0)\n",
    "\n",
    "# Create an empty RGB image\n",
    "dominant_image = np.zeros((*dominant_layer_indices.shape, 3), dtype=np.uint8)\n",
    "\n",
    "# Map each index to its corresponding color in the RGB image\n",
    "for index, color in color_map.items():\n",
    "    # Find where the dominant index is equal to the current layer index\n",
    "    mask = dominant_layer_indices == index\n",
    "    dominant_image[mask] = np.array(to_rgb(color)) * 255  # Convert color to RGB and adjust to a scale of 0-255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Land cover colors\n",
    "color_map = {\n",
    "    0: 'saddlebrown',   # Bare\n",
    "    1: 'red',           # BuiltUp\n",
    "    2: 'yellow',        # Crops\n",
    "    3: 'lightgreen',    # Grass\n",
    "    4: 'limegreen',     # MossLichen\n",
    "    5: 'blue',          # PermanentWater\n",
    "    6: 'cyan',          # SeasonalWater\n",
    "    7: 'olive',         # Shrub\n",
    "    8: 'darkgreen'      # Tree\n",
    "}\n",
    "\n",
    "# land cover labels\n",
    "labels = {\n",
    "    0: \"Bare\",\n",
    "    1: \"Built-Up\",\n",
    "    2: \"Crops\",\n",
    "    3: \"Grass\",\n",
    "    4: \"Moss/Lichen\",\n",
    "    5: \"Permanent water\",\n",
    "    6: \"Seasonal water\",\n",
    "    7: \"Shrub\",\n",
    "    8: \"Tree\"\n",
    "}\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(dominant_image)\n",
    "ax.axis('on')\n",
    "\n",
    "# Create a list of patches for the legend.\n",
    "patches = [mpatches.Patch(color=to_rgb(color), label=labels[idx]) for idx, color in color_map.items()]\n",
    "\n",
    "# Add legend\n",
    "fig.legend(handles=patches, loc='lower center', bbox_to_anchor=(0.5, 0.1), ncol=3, frameon=False, fontsize=13)\n",
    "\n",
    "# Adjust lengend's margins\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "ax.xaxis.set_ticks([])\n",
    "ax.yaxis.set_ticks([])\n",
    "\n",
    "# save the figure\n",
    "fig.savefig('../../Figures/figB3a.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4322027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot showing CNN activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b29083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(layer, input, output):\n",
    "    global activation\n",
    "    activation = torch.relu(output)  # Use ReLU para visualizar mejor las activaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a hook\n",
    "hook = model.conv1.register_forward_hook(get_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass to obtain the activations\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the hook after using it to prevent memory leaks\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d625ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that activations are defined\n",
    "if 'activation' in globals():\n",
    "    # Show feature plots\n",
    "    num_plots = activation.shape[1]\n",
    "    fig, axes = plt.subplots((num_plots + 3) // 4, 4, figsize=(12, (num_plots + 3) // 4 * 3))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_plots:\n",
    "            ax.imshow(activation[0, i].detach().numpy(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No activation data was recorded.\")\n",
    "    \n",
    "# Save figure\n",
    "fig.savefig('../../Figures/figA3.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9115bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get grad-CAM for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, layer):\n",
    "        self.model = model\n",
    "        self.layer = layer\n",
    "        self.gradient = None\n",
    "        self.activation = None\n",
    "\n",
    "        self.hook_handles = []\n",
    "        self.hook_handles.append(layer.register_forward_hook(self.save_activation))\n",
    "        self.hook_handles.append(layer.register_backward_hook(self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activation = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradient = grad_output[0].detach()\n",
    "\n",
    "    def __call__(self, x, index=None):\n",
    "        # Set a fixed seed for reproducibility\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "\n",
    "        # Clear previous gradients and activations\n",
    "        self.gradient = None\n",
    "        self.activation = None\n",
    "        \n",
    "        output = self.model(x)\n",
    "        if index is None:\n",
    "            index = torch.argmax(output)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        output.backward(torch.ones_like(output), retain_graph=True)\n",
    "\n",
    "        pooled_gradients = torch.mean(self.gradient, dim=[0, 2, 3])\n",
    "        for i in range(pooled_gradients.size(0)):\n",
    "            self.activation[:, i, :, :] *= pooled_gradients[i]\n",
    "        \n",
    "        heatmap = torch.mean(self.activation, dim=1).squeeze()\n",
    "        heatmap = F.relu(heatmap)\n",
    "        heatmap /= torch.max(heatmap)\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "    def release(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GradCAM on the model with the conv6 layer\n",
    "grad_cam = GradCAM(model, model.conv6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e87cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed seed to ensure reproducibility\n",
    "seed = 42\n",
    "# Get grad-CAM map\n",
    "heatmap = grad_cam(input_tensor)\n",
    "grad_cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7300ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Plot grad-CAM heatmap\n",
    "heatmap_np = heatmap.cpu().numpy()  # Convert the tensor to a NumPy array for visualization\n",
    "\n",
    "# Create the figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[20, 1]) \n",
    "\n",
    "ax = fig.add_subplot(gs[0])\n",
    "cax = ax.imshow(heatmap_np, cmap='hot')\n",
    "\n",
    "# Hide axis ticks and labels\n",
    "ax.axis('off')\n",
    "# Create the subplot for the color bar below the heatmap\n",
    "cbar_ax = fig.add_subplot(gs[1])\n",
    "\n",
    "# Add the color bar\n",
    "cbar = fig.colorbar(cax, cax=cbar_ax, orientation='horizontal')\n",
    "cbar_ax.xaxis.set_ticks_position('bottom')\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig('../../Figures/figB1.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba828c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5797055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_rgb\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "normalized_heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "# Create an RGBA image for `dominant_image` where the alpha channel is adjusted according to grad-CAM\n",
    "dominant_rgba_image = np.zeros((*dominant_image.shape[:2], 4), dtype=np.float32)  # Create RGBA image\n",
    "\n",
    "for i in range(3):  # Copy the RGB channels\n",
    "    dominant_rgba_image[..., i] = dominant_image[..., i] / 255.0  # Normalize and copy\n",
    "dominant_rgba_image[..., 3] = normalized_heatmap  # Adjust the alpha channel using the heatmap\n",
    "\n",
    "# Create image\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(dominant_rgba_image)\n",
    "ax.axis('on')\n",
    "\n",
    "# Define the color palette and labels for the legend of `dominant_image`\n",
    "color_map = {\n",
    "    \"Bare\": 'saddlebrown',\n",
    "    \"Built-Up\": 'red',\n",
    "    \"Crops\": 'yellow',\n",
    "    \"Grass\": 'lightgreen',\n",
    "    \"Moss/Lichen\": 'limegreen',\n",
    "    \"Permanent water\": 'blue',\n",
    "    \"Seasonal water\": 'cyan',\n",
    "    \"Shrub\": 'olive',\n",
    "    \"Tree\": 'darkgreen'\n",
    "}\n",
    "patches = [mpatches.Patch(color=to_rgb(color), label=label) for label, color in color_map.items()]\n",
    "\n",
    "# Add the legend to the plot\n",
    "fig.legend(handles=patches, loc='lower center', bbox_to_anchor=(0.5, 0.1), ncol=3, frameon=False, fontsize=13)\n",
    "\n",
    "# Adjust the margins to make space for the legend\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "ax.xaxis.set_ticks([]) \n",
    "ax.yaxis.set_ticks([])\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('../../Figures/figB3b.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20033a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f892b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will estimate the groups of important pixels that are together based on grad-CAM > 0 and the dominant land covers map\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_rgb\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Ensure the grad-CAM heatmap is normalized\n",
    "normalized_heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "# Create an RGBA image for `dominant_image` where the alpha channel is adjusted according to the heatmap\n",
    "dominant_rgba_image = np.zeros((*dominant_image.shape[:2], 4), dtype=np.float32)  # Create RGBA image\n",
    "\n",
    "for i in range(3):  # Copy the RGB channels\n",
    "    dominant_rgba_image[..., i] = np.around(dominant_image[..., i] / 255.0, 7)  # Normalize and copy\n",
    "\n",
    "########################################################\n",
    "# Adjust the alpha channel using the heatmap with the threshold\n",
    "threshold = 0.0\n",
    "#######################################################\n",
    "\n",
    "alpha_channel = np.where(normalized_heatmap > threshold, 1.0, 0)\n",
    "dominant_rgba_image[..., 3] = alpha_channel  # Apply the modified alpha channel\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.imshow(dominant_rgba_image)\n",
    "ax.axis('on')\n",
    "\n",
    "# Define the color palette and labels for the `dominant_image` legend\n",
    "color_map = {\n",
    "    \"Bare\": 'saddlebrown',\n",
    "    \"BuiltUp\": 'red',\n",
    "    \"Crops\": 'yellow',\n",
    "    \"Grass\": 'lightgreen',\n",
    "    \"MossLichen\": 'limegreen',\n",
    "    \"PermanentWater\": 'blue',\n",
    "    \"SeasonalWater\": 'cyan',\n",
    "    \"Shrub\": 'olive',\n",
    "    \"Tree\": 'darkgreen'\n",
    "}\n",
    "patches = [mpatches.Patch(color=to_rgb(color), label=label) for label, color in color_map.items()]\n",
    "\n",
    "ax.xaxis.set_ticks([])\n",
    "ax.yaxis.set_ticks([])\n",
    "\n",
    "# Add the legend for the land cover\n",
    "legend = ax.legend(handles=patches, loc='upper left', title=\"Land Cover Types\", bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "alpha_mask = dominant_rgba_image[..., 3] == 1\n",
    "\n",
    "# Create an output image that will be a copy of the original image\n",
    "output_image = np.zeros_like(dominant_rgba_image)\n",
    "\n",
    "# Copy only the pixels where alpha is 1\n",
    "output_image[alpha_mask] = dominant_rgba_image[alpha_mask]\n",
    "\n",
    "# Visualize the resulting image, which will now highlight the pixels with alpha = 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(output_image)\n",
    "plt.title(\"Pixels with Alpha = 1\")\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a76e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import measure, color\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the RGB image (ignoring alpha) to a binary image where there are non-black pixels\n",
    "binary_image = np.any(output_image[..., :3] > 0, axis=-1)  # True where there is color\n",
    "\n",
    "# Apply connected component labeling on the binary image\n",
    "labeled_image = measure.label(binary_image, connectivity=2)  # Diagonal connections included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure that we only use the RGB channels\n",
    "if output_image.shape[-1] == 4:  # If it still includes the alpha channel\n",
    "    output_image = output_image[..., :3]  # Take only the first three channels (RGB)\n",
    "\n",
    "# Apply label2rgb again\n",
    "from skimage import color\n",
    "\n",
    "image_labeled = color.label2rgb(labeled_image, output_image, kind='overlay', bg_label=0)\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.imshow(image_labeled)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('../../Figures/figB3c.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
