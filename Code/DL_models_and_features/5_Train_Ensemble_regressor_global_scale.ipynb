{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e26205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7182a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ml info\n",
    "df_extra_features_train_raw = pd.read_csv('../../Data/ml_train.csv')\n",
    "df_extra_features_test_raw = pd.read_csv('../../Data/ml_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra_features_test_raw = pd.read_csv('../../Data/ml_test.csv')# select those columns that Angel used to fit the ML models\n",
    "selected_columns = ['study_id','site_id','bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub','management','x0_1.0','x0_2.0','x0_4.0','x0_5.0','x0_7.0','x0_8.0','x0_10.0','x0_12.0']\n",
    "\n",
    "df_extra_features_train = df_extra_features_train_raw[selected_columns]\n",
    "df_extra_features_test = df_extra_features_test_raw[selected_columns]\n",
    "\n",
    "# Note: it seems that these partitions are different from those used to calibrate DI-ML\n",
    "# df_train.shape : (790, 25)\n",
    "# df_test.shape : (217, 25)\n",
    "\n",
    "df_ML_extra_features_info = pd.concat([df_extra_features_train, df_extra_features_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cde1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_global_regressor = pd.read_csv('../../Data/data_train_global_regressor_V0.csv')\n",
    "df_test_global_regressor = pd.read_csv('../../Data/data_test_global_regressor_V0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_train = df_train_global_regressor.merge(df_ML_extra_features_info, on=['study_id','site_id'], how='left')\n",
    "df_ML_test = df_test_global_regressor.merge(df_ML_extra_features_info, on=['study_id','site_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_train.to_csv('../../Data/df_ML_train_global_regressor_V0.csv', index=False)\n",
    "df_ML_test.to_csv('../../Data/df_ML_test_global_regressor_V0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175c34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "rows_with_na = df_ML_train.isna().any(axis=1)\n",
    "number_rows_with_na = rows_with_na.sum()\n",
    "print(number_rows_with_na) # 27 in the training set\n",
    "\n",
    "# sanity check\n",
    "rows_with_na = df_ML_test.isna().any(axis=1)\n",
    "number_rows_with_na = rows_with_na.sum()\n",
    "print(number_rows_with_na) # 20 in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs from dataframes\n",
    "df_ML_train_without_na = df_ML_train.dropna()\n",
    "df_ML_test_without_na = df_ML_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b23a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dec36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for cross validation\n",
    "df_folds = pd.read_csv('../../Data/CV_folds.csv')\n",
    "df_ML_train_studyid_series = df_ML_train_without_na['study_id']\n",
    "df_ML_train_studyid = df_ML_train_studyid_series.to_frame(name='study_id')\n",
    "df_ML_train_folds = df_ML_train_studyid.merge(df_folds, on=['study_id'], how='left')\n",
    "\n",
    "myCViterator = []\n",
    "for i in range(0,5):\n",
    "    trainIndices = df_ML_train_folds[df_ML_train_folds['fold'] != i].index.values.astype(int)\n",
    "    testIndices = df_ML_train_folds[df_ML_train_folds['fold'] == i].index.values.astype(int)\n",
    "    myCViterator.append((trainIndices, testIndices))\n",
    "\n",
    "# Save iterator    \n",
    "with open('../../Data/myCViterator_PREVENT.pkl', 'wb') as f:\n",
    "    pickle.dump(myCViterator, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our cv iterator (K-Folds)\n",
    "with open('../../Data/myCViterator_PREVENT.pkl', 'rb') as file:\n",
    "        myCViterator = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5deaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6fc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import spearmanr, uniform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define columns for CNN and non-CNN features\n",
    "X_columns_CNN = ['global_regressor_V0','bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub','management_y','x0_1.0','x0_2.0','x0_4.0','x0_5.0','x0_7.0','x0_8.0','x0_10.0','x0_12.0']\n",
    "X_columns_NO_CNN = ['bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub','management_y','x0_1.0','x0_2.0','x0_4.0','x0_5.0','x0_7.0','x0_8.0','x0_10.0','x0_12.0']\n",
    "\n",
    "y_columns = ['log_vr_total']\n",
    "\n",
    "# Select training and test sets\n",
    "X_train_CNN = df_ML_train_without_na[X_columns_CNN]\n",
    "X_train_NO_CNN = df_ML_train_without_na[X_columns_NO_CNN]\n",
    "X_test_CNN = df_ML_test_without_na[X_columns_CNN]\n",
    "X_test_NO_CNN = df_ML_test_without_na[X_columns_NO_CNN]\n",
    "\n",
    "# Select columns by name\n",
    "columns_to_scale_CNN = ['global_regressor_V0','bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub']\n",
    "columns_to_scale_NO_CNN = ['bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub']\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler_CNN = StandardScaler()\n",
    "scaler_NO_CNN = StandardScaler()\n",
    "\n",
    "# Fit the scaler only with the training data and then transform the training data\n",
    "X_train_CNN[columns_to_scale_CNN] = scaler_CNN.fit_transform(X_train_CNN[columns_to_scale_CNN])\n",
    "X_train_NO_CNN[columns_to_scale_NO_CNN] = scaler_NO_CNN.fit_transform(X_train_NO_CNN[columns_to_scale_NO_CNN])\n",
    "\n",
    "# Apply the transformation to the test data using the fitted values from the training data\n",
    "X_test_CNN[columns_to_scale_CNN] = scaler_CNN.transform(X_test_CNN[columns_to_scale_CNN])\n",
    "X_test_NO_CNN[columns_to_scale_NO_CNN] = scaler_NO_CNN.transform(X_test_NO_CNN[columns_to_scale_NO_CNN])\n",
    "\n",
    "# Select target variables\n",
    "y_train = df_ML_train_without_na[y_columns]\n",
    "y_test = df_ML_test_without_na[y_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model_bayesian_ridge = BayesianRidge(n_iter=25000)\n",
    "\n",
    "# Define the hyperparameter grid to explore\n",
    "param_grid = {\n",
    "    'alpha_1': uniform(loc=0, scale=1),\n",
    "    'alpha_2': uniform(loc=0, scale=1),\n",
    "    'lambda_1': uniform(loc=0, scale=1),\n",
    "    'lambda_2': uniform(loc=0, scale=1),\n",
    "    'fit_intercept': [False, True]\n",
    "}\n",
    "\n",
    "# Cross validation strategy (here we use K-Fold)\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "# Random search of best hyperparameter combinations\n",
    "grid_search = RandomizedSearchCV(model_bayesian_ridge, param_distributions = param_grid, cv = myCViterator,\n",
    "                            scoring = 'neg_mean_absolute_error', n_iter = 10000,\n",
    "                            verbose = 2, random_state = 135, n_jobs = 8)\n",
    "\n",
    "grid_search.fit(X_train_CNN, y_train)\n",
    "\n",
    "# best hyperparameter combination\n",
    "best_hyperparameters_CNN = grid_search.best_params_\n",
    "\n",
    "# best model\n",
    "best_model_CNN = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3649d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_CNN = best_model_CNN.predict(X_test_CNN)\n",
    "\n",
    "# Evaluation of our best model\n",
    "mae = mean_absolute_error(y_test, y_pred_CNN)\n",
    "mse = mean_squared_error(y_test, y_pred_CNN)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_CNN)\n",
    "\n",
    "# Estimate spearman rho\n",
    "coef_spearman, p_valor_spearman = spearmanr(y_test, y_pred_CNN)\n",
    "\n",
    "# Best model results\n",
    "# print(\"Model coefficients:\", best_model_CNN.coef_)\n",
    "# print(\"Intercept:\", best_model_CNN.intercept_)\n",
    "print(\"Mean square error (MSE):\", mse)\n",
    "print(\"Root mean square error (RMSE):\", rmse)\n",
    "print(\"Mean absolute error (MAE):\", mae)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Spearman-rho:\", coef_spearman,\"(pvalor:\",p_valor_spearman,\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_true = y_test.to_numpy()\n",
    "y_true_array = y_true.flatten()\n",
    "# fit a regression line with numpy\n",
    "coef = np.polyfit(y_true_array, y_pred_CNN, 1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "\n",
    "# create a scatter plot with regression line\n",
    "plt.scatter(y_true_array, y_pred_CNN, label='Data')\n",
    "plt.plot(y_true_array, poly1d_fn(y_true_array), color = 'red', label = 'Linear regression')\n",
    "plt.plot(sorted(y_true_array), sorted(y_true_array), color = 'blue', linestyle = '--', label = 'Expected trend')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Observed visitation rate')\n",
    "plt.ylabel('Predicted visitation rate')\n",
    "plt.title('Results for BayRidge regressor with the CNN results at global scale')\n",
    "\n",
    "# show legend\n",
    "plt.legend()\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coef. from the best model estimated\n",
    "coeficients = best_model_CNN.coef_\n",
    "\n",
    "# Get variance from the estimated coeficients\n",
    "variance_coefs = np.var(coeficients)\n",
    "\n",
    "# Estimate VIP for each variable\n",
    "VIP = np.abs(coeficients) / variance_coefs\n",
    "\n",
    "# Sort VIP\n",
    "sorted_indices = np.argsort(VIP)[::-1]  # Sort from larger to smaller VIP\n",
    "sorted_VIP = VIP[sorted_indices]\n",
    "\n",
    "# Print VIP\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    print(f\"{X_columns_CNN[idx]}: VIP = {sorted_VIP[i]}\")\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_indices)), sorted_VIP, align='center')\n",
    "plt.yticks(range(len(sorted_indices)), [X_columns_CNN[i] for i in sorted_indices])\n",
    "plt.xlabel('VIP (Variable Importance Plot)')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Variable Importance Plot for the Bayesian Ridge Regression model with CNN info')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d2cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27289a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196abe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd28e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and using grandient boost??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we eliminate highly correlated variables before fitting the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df, columns):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df[columns].values, i) for i in range(len(columns))]\n",
    "    return vif_data\n",
    "\n",
    "X_columns_CNN_raw = ['global_regressor_V0','bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub','management_y','x0_1.0','x0_2.0','x0_4.0','x0_5.0','x0_7.0','x0_8.0','x0_10.0','x0_12.0']\n",
    "X_columns_NO_CNN_raw = ['bio01','bio02','bio05','bio08','bio14','ec','elevation','es','gHM','pdsi','soil_den_b10','moss','shrub','management_y','x0_1.0','x0_2.0','x0_4.0','x0_5.0','x0_7.0','x0_8.0','x0_10.0','x0_12.0']\n",
    "\n",
    "y_columns = ['log_vr_total']\n",
    "\n",
    "X_train_CNN_raw = df_ML_train_without_na[X_columns_CNN_raw]\n",
    "X_train_NO_CNN_raw = df_ML_train_without_na[X_columns_NO_CNN_raw]\n",
    "X_test_CNN_raw = df_ML_test_without_na[X_columns_CNN_raw]\n",
    "X_test_NO_CNN_raw = df_ML_test_without_na[X_columns_NO_CNN_raw]\n",
    "\n",
    "\n",
    "# Estimate VIF for X_train_CNN\n",
    "vif_CNN = calculate_vif(X_train_CNN_raw, X_columns_CNN_raw)\n",
    "print(\"VIF for X_train_CNN:\")\n",
    "print(vif_CNN)\n",
    "\n",
    "# Estimate VIF for X_train_NO_CNN\n",
    "vif_NO_CNN = calculate_vif(X_train_NO_CNN_raw, X_columns_NO_CNN_raw)\n",
    "print(\"VIF for X_train_NO_CNN:\")\n",
    "print(vif_NO_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for VIF\n",
    "vif_threshold = 8\n",
    "\n",
    "# Select columns that have VIF less than the threshold\n",
    "selected_columns_CNN_raw = vif_CNN[vif_CNN[\"VIF\"] < vif_threshold][\"feature\"].tolist()\n",
    "selected_columns_NO_CNN_raw = vif_NO_CNN[vif_NO_CNN[\"VIF\"] < vif_threshold][\"feature\"].tolist()\n",
    "\n",
    "# Create new dataframes with the selected columns\n",
    "X_train_CNN_raw_reduced = X_train_CNN_raw[selected_columns_CNN_raw]\n",
    "X_train_NO_CNN_raw_reduced = X_train_NO_CNN_raw[selected_columns_NO_CNN_raw]\n",
    "X_test_CNN_raw_reduced = X_test_CNN_raw[selected_columns_CNN_raw]\n",
    "X_test_NO_CNN_raw_reduced = X_test_NO_CNN_raw[selected_columns_NO_CNN_raw]\n",
    "\n",
    "print(\"Columns selected for X_train_CNN:\", selected_columns_CNN_raw)\n",
    "print(\"Columns selected for X_train_NO_CNN:\", selected_columns_NO_CNN_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale columns selected \n",
    "scaler_CNN_raw = StandardScaler()\n",
    "scaler_NO_CNN_raw = StandardScaler()\n",
    "\n",
    "# Fit the scaler with just the training data and then transform the training data\n",
    "X_train_CNN_raw_reduced_sc = scaler_CNN_raw.fit_transform(X_train_CNN_raw_reduced)\n",
    "X_train_NO_CNN_raw_reduced_sc = scaler_NO_CNN_raw.fit_transform(X_train_NO_CNN_raw_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_CNN_raw_reduced_sc, y_train)\n",
    "\n",
    "# best hyperparameter combination\n",
    "best_hyperparameters_CNN = grid_search.best_params_\n",
    "\n",
    "# best model\n",
    "best_model_CNN_reduced = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb094698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_CNN_raw_reduced = best_model_CNN_reduced.predict(X_test_CNN_raw_reduced)\n",
    "\n",
    "# Evaluation of our best new model\n",
    "mae_raw_reduced = mean_absolute_error(y_test, y_pred_CNN_raw_reduced)\n",
    "mse_raw_reduced = mean_squared_error(y_test, y_pred_CNN_raw_reduced)\n",
    "rmse_raw_reduced = np.sqrt(mse_raw_reduced)\n",
    "r2_raw_reduced = r2_score(y_test, y_pred_CNN_raw_reduced)\n",
    "\n",
    "# Estimate spearman rho\n",
    "coef_spearman_raw_reduced, p_valor_spearman_raw_reduced = spearmanr(y_test, y_pred_CNN_raw_reduced)\n",
    "\n",
    "# Best model results\n",
    "# print(\"Model coefficients:\", best_model_CNN.coef_)\n",
    "# print(\"Intercept:\", best_model_CNN.intercept_)\n",
    "print(\"Mean square error (MSE):\", mse_raw_reduced)\n",
    "print(\"Root mean square error (RMSE):\", rmse_raw_reduced)\n",
    "print(\"Mean absolute error (MAE):\", mae_raw_reduced)\n",
    "print(\"R-squared:\", r2_raw_reduced)\n",
    "print(\"Spearman-rho:\", coef_spearman_raw_reduced,\"(pvalor:\",p_valor_spearman_raw_reduced,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42b7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0e698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9db99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the CNN prediction introducing noise??? it's possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model without CNN info (global regressor)\n",
    "grid_search.fit(X_train_NO_CNN, y_train)\n",
    "\n",
    "# best hyperparameter combination\n",
    "best_hyperparameters_NO_CNN = grid_search.best_params_\n",
    "\n",
    "# best model\n",
    "best_model_NO_CNN = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c229084",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_NO_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_NO_CNN = best_model_NO_CNN.predict(X_test_NO_CNN)\n",
    "\n",
    "# Evaluation of our best model\n",
    "mae_NO_CNN = mean_absolute_error(y_test, y_pred_NO_CNN)\n",
    "mse_NO_CNN = mean_squared_error(y_test, y_pred_NO_CNN)\n",
    "rmse_NO_CNN = np.sqrt(mse_NO_CNN)\n",
    "r2_NO_CNN = r2_score(y_test, y_pred_NO_CNN)\n",
    "\n",
    "# Estimate spearman rho\n",
    "coef_spearman_NO_CNN, p_valor_spearman_NO_CNN = spearmanr(y_test, y_pred_NO_CNN)\n",
    "\n",
    "# Best model results\n",
    "# print(\"Model coefficients:\", best_model_NO_CNN.coef_)\n",
    "# print(\"Intercept:\", best_model_NO_CNN.intercept_)\n",
    "print(\"Mean square error (MSE):\", mse_NO_CNN)\n",
    "print(\"Root mean square error (RMSE):\", rmse_NO_CNN)\n",
    "print(\"R-squared:\", r2_NO_CNN)\n",
    "print(\"Mean absolute error (MAE):\", mae_NO_CNN)\n",
    "print(\"Spearman-rho:\", coef_spearman_NO_CNN,\"(pvalor:\",p_valor_spearman_NO_CNN,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5262f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_true = y_test.to_numpy()\n",
    "y_true_array = y_true.flatten()\n",
    "# fit a regression line with numpy\n",
    "coef = np.polyfit(y_true_array, y_pred_NO_CNN, 1)\n",
    "poly1d_fn = np.poly1d(coef)\n",
    "\n",
    "# create a scatter plot with regression line\n",
    "plt.scatter(y_true_array, y_pred_NO_CNN, label='Data')\n",
    "plt.plot(y_true_array, poly1d_fn(y_true_array), color = 'red', label = 'Linear regression')\n",
    "plt.plot(sorted(y_true_array), sorted(y_true_array), color = 'blue', linestyle = '--', label = 'Expected trend')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('Observed visitation rate')\n",
    "plt.ylabel('Predicted visitation rate')\n",
    "plt.title('Results for BayRidge regressor without CNN info at global scale')\n",
    "\n",
    "# show legend\n",
    "plt.legend()\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedf36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coef. from the best model estimated\n",
    "coeficients_NO_CNN = best_model_NO_CNN.coef_\n",
    "\n",
    "# Get variance from the estimated coeficients\n",
    "variance_coefs_NO_CNN = np.var(coeficients_NO_CNN)\n",
    "\n",
    "# Estimate VIP for each variable\n",
    "VIP_NO_CNN = np.abs(coeficients_NO_CNN) / variance_coefs_NO_CNN\n",
    "\n",
    "# Sort VIP\n",
    "sorted_indices_NO_CNN = np.argsort(VIP_NO_CNN)[::-1]  # Sort from larger to smaller VIP\n",
    "sorted_VIP_NO_CNN = VIP_NO_CNN[sorted_indices_NO_CNN]\n",
    "\n",
    "# Print VIP\n",
    "for i, idx in enumerate(sorted_indices_NO_CNN):\n",
    "    print(f\"{X_columns_NO_CNN[idx]}: VIP = {sorted_VIP_NO_CNN[i]}\")\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_indices_NO_CNN)), sorted_VIP_NO_CNN, align='center')\n",
    "plt.yticks(range(len(sorted_indices_NO_CNN)), [X_columns_NO_CNN[i] for i in sorted_indices_NO_CNN])\n",
    "plt.xlabel('VIP (Variable Importance Plot)')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Variable Importance Plot for the Bayesian Ridge Regression model WITHOUT CNN info')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aaf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_NO_CNN = best_model_NO_CNN.predict(X_test_NO_CNN)\n",
    "df_ML_test_without_na['BayReg'] = y_pred_test_NO_CNN.flatten()\n",
    "\n",
    "y_pred_train_NO_CNN = best_model_NO_CNN.predict(X_train_NO_CNN)\n",
    "df_ML_train_without_na['BayReg'] = y_pred_train_NO_CNN.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe429993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's adjust by adding the BayReg data and the rest of the Bayreg+CNN variables\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Add BayReg predictions as a new column\n",
    "X_train_CNN_Bay_others = np.hstack((X_train_CNN, y_pred_train_NO_CNN.reshape(-1, 1)))\n",
    "X_test_CNN_Bay_others = np.hstack((X_test_CNN, y_pred_test_NO_CNN.reshape(-1, 1)))\n",
    "\n",
    "# Scale features\n",
    "scaler_CNN_Bay_others = StandardScaler()\n",
    "\n",
    "# Adjust and transform the training set\n",
    "X_train_CNN_Bay_others = scaler_CNN_Bay_others.fit_transform(X_train_CNN_Bay_others)\n",
    "\n",
    "# Adjust and transform the test set\n",
    "X_test_CNN_Bay_others = scaler_CNN_Bay_others.transform(X_test_CNN_Bay_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c8056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to do an optimized search for parameters for the neural regressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a neural network model with batch normalization and dropout\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create a NeuralNetRegressor\n",
    "net = NeuralNetRegressor(\n",
    "    NeuralNetwork,\n",
    "    module__input_dim=X_train_CNN_Bay_others.shape[1],\n",
    "    max_epochs=50,\n",
    "    lr=0.01,\n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "params = {\n",
    "    'lr': [0.1],\n",
    "    'max_epochs': [50],\n",
    "    'module__hidden_dim1': [32],\n",
    "    'module__hidden_dim2': [32],\n",
    "    'module__dropout_rate': [.679,.68,.685,.689,.69,.691],\n",
    "}\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_CNN_Bay_others, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_CNN_Bay_others, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "gs = GridSearchCV(net, params, refit=True, cv=myCViterator, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "gs.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_test = gs.predict(X_test_tensor)\n",
    "\n",
    "# Calculate the Spearman coefficient\n",
    "rho = spearmanr(y_test, y_pred_test.flatten())[0]\n",
    "print(f\"Spearman's rho: {rho:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bef749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find NN model with a higher Spearman rho value than the BayRidge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "def find_best_model(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, net, params, myCViterator, rho_threshold):\n",
    "    rho = 0  # Initialize rho\n",
    "    best_params = None\n",
    "\n",
    "    while rho <= rho_threshold:\n",
    "        # Search hyperparameters\n",
    "        gs = GridSearchCV(net, params, refit=True, cv=myCViterator, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "        gs.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "        # Eval model\n",
    "        y_pred_test = gs.predict(X_test_tensor)\n",
    "\n",
    "        # Estimate its Spearman's rho\n",
    "        rho = spearmanr(y_test_tensor, y_pred_test.flatten())[0]\n",
    "        print(f\"Spearman's rho en el conjunto de test: {rho:.4f}\")\n",
    "\n",
    "        if rho > rho_threshold:\n",
    "            best_params = gs.best_params_\n",
    "            break\n",
    "\n",
    "    return best_params, rho\n",
    "\n",
    "best_params, rho = find_best_model(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, net, params, myCViterator, coef_spearman_NO_CNN)\n",
    "\n",
    "print(\"best hyperparameter found:\")\n",
    "print(best_params)\n",
    "print(f\"Best Spearman's rho: {rho:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5010e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the variables necessary to generate Figure 1b and evaluate the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ensemble model (commented for security reasons)\n",
    "# torch.save(gs.best_estimator_.module_.state_dict(), '../../Data/Calibrated_models/ensemble_model_V0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1caebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../Data/X_train_CNN_Bay_others.npy', X_train_CNN_Bay_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fe420",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_test_tensor, '../../Data/X_test_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(y_test_tensor, '../../Data/y_test_tensor.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
