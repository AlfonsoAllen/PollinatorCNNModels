{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4270c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Usar GPU\n",
    "    print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Usar CPU\n",
    "    print(\"GPU no disponible, se utilizará la CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a53a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d135609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "number_raster_layers = 9\n",
    "number_epoch = 1000\n",
    "radius_maps = \"1050\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1604b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6962c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training and test info\n",
    "\n",
    "df_train = pd.read_csv('../../Data/data_train_new.csv')\n",
    "df_test = pd.read_csv('../../Data/data_test_new.csv')\n",
    "\n",
    "rows_with_na = df_test.isna().any(axis=1)\n",
    "indices_na = df_test.index[rows_with_na]\n",
    "df_test = df_test.drop(index=indices_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_training_sites = df_train.shape[0]\n",
    "number_test_sites = df_test.shape[0]\n",
    "\n",
    "print(\"Training sites:\",number_training_sites)\n",
    "print(\"Test sites:\",number_test_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37987f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training/test values and sites\n",
    "train_values = df_train[\"log_vr_total\"][0:number_training_sites]\n",
    "train_sites = df_train[\"plot_id\"][0:number_training_sites]\n",
    "\n",
    "test_values = df_test[\"log_vr_total\"][0:number_test_sites]\n",
    "test_sites = df_test[\"plot_id\"][0:number_test_sites]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd997a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the response variable into tensors\n",
    "y_train = torch.tensor(np.repeat(train_values.values,4)) # We repeat each value 4 times because we included the raster and 3 rotations\n",
    "y_test = torch.tensor(test_values.values) # same thing here\n",
    "y_train = y_train.unsqueeze(1)\n",
    "y_test = y_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the minimum number of pixels per layer\n",
    "min_rows_train = 100000\n",
    "min_columns_train = 100000\n",
    "min_rows_test = 100000\n",
    "min_columns_test = 100000\n",
    "\n",
    "for site in train_sites:\n",
    "    \n",
    "    # print(site)\n",
    "    \n",
    "    data1 = pd.read_csv('../../Data/Copernicus_maps/Bare_df/radius_' + radius_maps + '/' + site + '.csv')\n",
    "    data2 = pd.read_csv('../../Data/Copernicus_maps/BuiltUp_df/radius_' + radius_maps + '/' + site + '.csv')\n",
    "    \n",
    "    rows_data1 = data1.shape[0]\n",
    "    columns_data1 = data1.shape[1]\n",
    "    \n",
    "    if rows_data1 < min_rows_train:\n",
    "        min_rows_train = rows_data1\n",
    "        \n",
    "    if columns_data1 < min_columns_train:\n",
    "        min_columns_train = columns_data1\n",
    "\n",
    "for site in test_sites:\n",
    "    \n",
    "    # print(site)\n",
    "    \n",
    "    data1 = pd.read_csv('../../Data/Copernicus_maps/Bare_df/radius_' + radius_maps + '/' + site + '.csv')\n",
    "    data2 = pd.read_csv('../../Data/Copernicus_maps/BuiltUp_df/radius_' + radius_maps + '/' + site + '.csv')\n",
    "    \n",
    "    rows_data1 = data1.shape[0]\n",
    "    columns_data1 = data1.shape[1]\n",
    "    \n",
    "    if rows_data1 < min_rows_test:\n",
    "        min_rows_test = rows_data1\n",
    "        \n",
    "    if columns_data1 < min_columns_test:\n",
    "        min_columns_test = columns_data1\n",
    "\n",
    "        \n",
    "print(min_rows_train)\n",
    "print(min_columns_train)\n",
    "print(min_rows_test)\n",
    "print(min_columns_test)\n",
    "\n",
    "number_pixels_layer = min(min_rows_train,min_rows_test,min_columns_train,min_columns_test)\n",
    "\n",
    "if number_pixels_layer % 2 == 0:\n",
    "    number_pixels_layer = number_pixels_layer - 1\n",
    "\n",
    "print('Minimum number of pixels per (square) layer:', number_pixels_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_dataframe(data,number_pixels_layer):\n",
    "    \n",
    "    # turn nans into zeros\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    rows_data = data.shape[0]\n",
    "    columns_data = data.shape[1]\n",
    "    \n",
    "    # if the number of rows/columns are even we remove first row/column\n",
    "    if rows_data % 2 == 0:\n",
    "        data = data.drop([0])\n",
    "        rows_data = data.shape[0]\n",
    "    if columns_data % 2 == 0:\n",
    "        data = data.drop(data.columns[0], axis=1)\n",
    "        columns_data = data.shape[1]\n",
    "        \n",
    "    # Reduce images a matrices with number_pixels_layer x number_pixels_layer pixels\n",
    "    \n",
    "    while columns_data != number_pixels_layer:\n",
    "        \n",
    "        data = data.drop(data.columns[0], axis=1)\n",
    "        columns_data = data.shape[1]\n",
    "        data = data.drop(data.columns[(columns_data-1)], axis=1)\n",
    "        columns_data = data.shape[1]\n",
    "        \n",
    "    while rows_data != number_pixels_layer:\n",
    "        \n",
    "        data = data.drop(data.index[[0, len(data)-1]])\n",
    "        rows_data = data.shape[0]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfa677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase the number of training maps we rotated them.\n",
    "\n",
    "def create_tensor_from_9_dataframe_layers(data1 ,data2, data3, data4, data5, data6, data7, data8, data9):\n",
    "    \n",
    "    # create tensor from data layer \n",
    "    tensor1 = torch.tensor(data1.values)\n",
    "    tensor2 = torch.tensor(data2.values)\n",
    "    tensor3 = torch.tensor(data3.values)\n",
    "    tensor4 = torch.tensor(data4.values)\n",
    "    tensor5 = torch.tensor(data5.values)\n",
    "    tensor6 = torch.tensor(data6.values)\n",
    "    tensor7 = torch.tensor(data7.values)\n",
    "    tensor8 = torch.tensor(data8.values)\n",
    "    tensor9 = torch.tensor(data9.values)\n",
    "        \n",
    "    return torch.stack((tensor1, tensor2, tensor3, tensor4, tensor5, tensor6, tensor7, tensor8, tensor9), dim=0)\n",
    "\n",
    "def create_tensor_from_9_dataframe_layers_rotated_90_clock(data1 ,data2, data3, data4, data5, data6, data7, data8, data9):\n",
    "    \n",
    "    # rotate layers (90º, clockwise) \n",
    "    data1 = np.rot90(data1, k = -1)\n",
    "    data2 = np.rot90(data2, k = -1)\n",
    "    data3 = np.rot90(data3, k = -1)\n",
    "    data4 = np.rot90(data4, k = -1)\n",
    "    data5 = np.rot90(data5, k = -1)\n",
    "    data6 = np.rot90(data6, k = -1)\n",
    "    data7 = np.rot90(data7, k = -1)\n",
    "    data8 = np.rot90(data8, k = -1)\n",
    "    data9 = np.rot90(data9, k = -1)\n",
    "    \n",
    "    data1 = pd.DataFrame(data1.copy())\n",
    "    data2 = pd.DataFrame(data2.copy())\n",
    "    data3 = pd.DataFrame(data3.copy())\n",
    "    data4 = pd.DataFrame(data4.copy())\n",
    "    data5 = pd.DataFrame(data5.copy())\n",
    "    data6 = pd.DataFrame(data6.copy())\n",
    "    data7 = pd.DataFrame(data7.copy())\n",
    "    data8 = pd.DataFrame(data8.copy())\n",
    "    data9 = pd.DataFrame(data9.copy())\n",
    "        \n",
    "    return create_tensor_from_9_dataframe_layers(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "\n",
    "def create_tensor_from_9_dataframe_layers_rotated_90_counter(data1 ,data2, data3, data4, data5, data6, data7, data8, data9):\n",
    "    \n",
    "    # rotate layers (90º, counterclockwise) \n",
    "    data1 = np.rot90(data1, k = 1)\n",
    "    data2 = np.rot90(data2, k = 1)\n",
    "    data3 = np.rot90(data3, k = 1)\n",
    "    data4 = np.rot90(data4, k = 1)\n",
    "    data5 = np.rot90(data5, k = 1)\n",
    "    data6 = np.rot90(data6, k = 1)\n",
    "    data7 = np.rot90(data7, k = 1)\n",
    "    data8 = np.rot90(data8, k = 1)\n",
    "    data9 = np.rot90(data9, k = 1)\n",
    "    \n",
    "    data1 = pd.DataFrame(data1.copy())\n",
    "    data2 = pd.DataFrame(data2.copy())\n",
    "    data3 = pd.DataFrame(data3.copy())\n",
    "    data4 = pd.DataFrame(data4.copy())\n",
    "    data5 = pd.DataFrame(data5.copy())\n",
    "    data6 = pd.DataFrame(data6.copy())\n",
    "    data7 = pd.DataFrame(data7.copy())\n",
    "    data8 = pd.DataFrame(data8.copy())\n",
    "    data9 = pd.DataFrame(data9.copy())\n",
    "        \n",
    "    return create_tensor_from_9_dataframe_layers(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "\n",
    "def create_tensor_from_9_dataframe_layers_rotated_180(data1 ,data2, data3, data4, data5, data6, data7, data8, data9):\n",
    "    \n",
    "     # create tensor from data layer \n",
    "    data1 = np.rot90(np.rot90(data1, k = -1), k = -1)\n",
    "    data2 = np.rot90(np.rot90(data2, k = -1), k = -1)\n",
    "    data3 = np.rot90(np.rot90(data3, k = -1), k = -1)\n",
    "    data4 = np.rot90(np.rot90(data4, k = -1), k = -1)\n",
    "    data5 = np.rot90(np.rot90(data5, k = -1), k = -1)\n",
    "    data6 = np.rot90(np.rot90(data6, k = -1), k = -1)\n",
    "    data7 = np.rot90(np.rot90(data7, k = -1), k = -1)\n",
    "    data8 = np.rot90(np.rot90(data8, k = -1), k = -1)\n",
    "    data9 = np.rot90(np.rot90(data9, k = -1), k = -1)\n",
    "    \n",
    "    data1 = pd.DataFrame(data1.copy())\n",
    "    data2 = pd.DataFrame(data2.copy())\n",
    "    data3 = pd.DataFrame(data3.copy())\n",
    "    data4 = pd.DataFrame(data4.copy())\n",
    "    data5 = pd.DataFrame(data5.copy())\n",
    "    data6 = pd.DataFrame(data6.copy())\n",
    "    data7 = pd.DataFrame(data7.copy())\n",
    "    data8 = pd.DataFrame(data8.copy())\n",
    "    data9 = pd.DataFrame(data9.copy())\n",
    "        \n",
    "    return create_tensor_from_9_dataframe_layers(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ee83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training tensor\n",
    "for site in train_sites:\n",
    "    \n",
    "    # print(site)\n",
    "    \n",
    "    data1 = pd.read_csv('../../Data/Copernicus_maps/Bare_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data2 = pd.read_csv('../../Data/Copernicus_maps/BuiltUp_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data3 = pd.read_csv('../../Data/Copernicus_maps/Crops_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data4 = pd.read_csv('../../Data/Copernicus_maps/Grass_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data5 = pd.read_csv('../../Data/Copernicus_maps/MossLichen_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data6 = pd.read_csv('../../Data/Copernicus_maps/PermanentWater_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data7 = pd.read_csv('../../Data/Copernicus_maps/SeasonalWater_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data8 = pd.read_csv('../../Data/Copernicus_maps/Shrub_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data9 = pd.read_csv('../../Data/Copernicus_maps/Tree_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    \n",
    "    data1 = adjust_dataframe(data1,number_pixels_layer)\n",
    "    data2 = adjust_dataframe(data2,number_pixels_layer)\n",
    "    data3 = adjust_dataframe(data3,number_pixels_layer)\n",
    "    data4 = adjust_dataframe(data4,number_pixels_layer)\n",
    "    data5 = adjust_dataframe(data5,number_pixels_layer)\n",
    "    data6 = adjust_dataframe(data6,number_pixels_layer)\n",
    "    data7 = adjust_dataframe(data7,number_pixels_layer)\n",
    "    data8 = adjust_dataframe(data8,number_pixels_layer)\n",
    "    data9 = adjust_dataframe(data9,number_pixels_layer)\n",
    "\n",
    "    # Create tensor from layers\n",
    "    tensor_9capas = create_tensor_from_9_dataframe_layers(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "    \n",
    "    # Create tensor from layers rotated 90º clockwise\n",
    "    tensor_9capas_rotated_90_clock = create_tensor_from_9_dataframe_layers_rotated_90_clock(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "    \n",
    "    # Create tensor from layers rotated 90º counterclockwise\n",
    "    tensor_9capas_rotated_90_counter = create_tensor_from_9_dataframe_layers_rotated_90_counter(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "    \n",
    "    # Create tensor from layers rotated 180º\n",
    "    tensor_9capas_rotated_180 = create_tensor_from_9_dataframe_layers_rotated_180(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "    \n",
    "    # print(tensor_9capas.shape)\n",
    "\n",
    "    tensor_9capas = tensor_9capas.unsqueeze(0)\n",
    "    tensor_9capas_rotated_90_clock = tensor_9capas_rotated_90_clock.unsqueeze(0)\n",
    "    tensor_9capas_rotated_90_counter = tensor_9capas_rotated_90_counter.unsqueeze(0)\n",
    "    tensor_9capas_rotated_180 = tensor_9capas_rotated_180.unsqueeze(0)\n",
    "    \n",
    "    # print(tensor_9capas.shape)\n",
    "    \n",
    "    if site == train_sites.iloc[0]:\n",
    "        \n",
    "        training_tensor = tensor_9capas\n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas_rotated_90_clock), dim=0)\n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas_rotated_90_counter), dim=0)\n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas_rotated_180), dim=0)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas), dim=0)\n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas_rotated_90_clock), dim=0)\n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas_rotated_90_counter), dim=0)\n",
    "        training_tensor = torch.cat((training_tensor, tensor_9capas_rotated_180), dim=0)\n",
    "        \n",
    "    \n",
    "print(training_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test tensor\n",
    "for site in test_sites:\n",
    "    \n",
    "    # print(site)\n",
    "    \n",
    "    data1 = pd.read_csv('../../Data/Copernicus_maps/Bare_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data2 = pd.read_csv('../../Data/Copernicus_maps/BuiltUp_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data3 = pd.read_csv('../../Data/Copernicus_maps/Crops_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data4 = pd.read_csv('../../Data/Copernicus_maps/Grass_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data5 = pd.read_csv('../../Data/Copernicus_maps/MossLichen_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data6 = pd.read_csv('../../Data/Copernicus_maps/PermanentWater_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data7 = pd.read_csv('../../Data/Copernicus_maps/SeasonalWater_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data8 = pd.read_csv('../../Data/Copernicus_maps/Shrub_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    data9 = pd.read_csv('../../Data/Copernicus_maps/Tree_df/radius_'+ radius_maps +'/' + site + '.csv')\n",
    "    \n",
    "    data1 = adjust_dataframe(data1,number_pixels_layer)\n",
    "    data2 = adjust_dataframe(data2,number_pixels_layer)\n",
    "    data3 = adjust_dataframe(data3,number_pixels_layer)\n",
    "    data4 = adjust_dataframe(data4,number_pixels_layer)\n",
    "    data5 = adjust_dataframe(data5,number_pixels_layer)\n",
    "    data6 = adjust_dataframe(data6,number_pixels_layer)\n",
    "    data7 = adjust_dataframe(data7,number_pixels_layer)\n",
    "    data8 = adjust_dataframe(data8,number_pixels_layer)\n",
    "    data9 = adjust_dataframe(data9,number_pixels_layer)\n",
    "\n",
    "    # Create tensor from layers\n",
    "    tensor_9capas = create_tensor_from_9_dataframe_layers(data1 ,data2, data3, data4, data5, data6, data7, data8, data9)\n",
    "\n",
    "    tensor_9capas = tensor_9capas.unsqueeze(0)\n",
    "    \n",
    "    if site == test_sites.iloc[0]:\n",
    "        \n",
    "        test_tensor = tensor_9capas\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        test_tensor = torch.cat((test_tensor, tensor_9capas), dim=0)\n",
    "        \n",
    "\n",
    "print(test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d14a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(number_raster_layers, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu1 = nn.SELU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu2 = nn.SELU()\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu3 = nn.SELU()\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu4 = nn.SELU()\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu5 = nn.SELU()\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.selu6 = nn.SELU()\n",
    "        #self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(512 * number_pixels_layer * number_pixels_layer, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.selu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.selu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.selu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.selu4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.selu5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.selu6(x)\n",
    "        x = self.flatten(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = CNNRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a15a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN: COMPUTERS MEMORY RUNS OUT\n",
    "## Check GPU availability\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#    print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#    print(\"GPU no disponible, se utilizará la CPU\")\n",
    "\n",
    "## Create a model and send it to GPUs\n",
    "#model = CNNRegressor().to(device)\n",
    "\n",
    "## Send data to GPUs\n",
    "#training_tensor = training_tensor.to(device)\n",
    "#y_train = y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c56c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimization\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee37a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b79ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for epoch in range(number_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(training_tensor.to(torch.float))\n",
    "    loss = criterion(outputs, y_train.to(torch.float))\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_lambda = 0.01  # L2 regularization factor\n",
    "    l2_regularization = torch.tensor(0.)  # init L2 regularization factor\n",
    "\n",
    "    for param in model.parameters():\n",
    "        l2_regularization += torch.norm(param, 9)  # add L2 norm of each parameter\n",
    "\n",
    "    loss += l2_lambda * l2_regularization  # add L2 reg to total loss\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{number_epoch}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a6472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67032721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted model\n",
    "path_model = '../../Data/Calibrated_models/global_regressor_V0.pth'\n",
    "torch.save(model.state_dict(), path_model)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
